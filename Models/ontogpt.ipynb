{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ontogpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:ontogpt.clients:llm_gpt4all module not found. GPT4All support will be disabled.\n",
      "WARNING:ontogpt.engines.knowledge_engine:GPT4All client not available. GPT4All support will be disabled.\n",
      "Traceback (most recent call last):\n",
      "  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"C:\\Users\\vanheesla\\AppData\\Local\\anaconda3\\envs\\OntoGPT\\Scripts\\ontogpt.exe\\__main__.py\", line 7, in <module>\n",
      "  File \"C:\\Users\\vanheesla\\AppData\\Local\\anaconda3\\envs\\OntoGPT\\Lib\\site-packages\\click\\core.py\", line 1157, in __call__\n",
      "    return self.main(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\vanheesla\\AppData\\Local\\anaconda3\\envs\\OntoGPT\\Lib\\site-packages\\click\\core.py\", line 1078, in main\n",
      "    rv = self.invoke(ctx)\n",
      "         ^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\vanheesla\\AppData\\Local\\anaconda3\\envs\\OntoGPT\\Lib\\site-packages\\click\\core.py\", line 1688, in invoke\n",
      "    return _process_result(sub_ctx.command.invoke(sub_ctx))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\vanheesla\\AppData\\Local\\anaconda3\\envs\\OntoGPT\\Lib\\site-packages\\click\\core.py\", line 1434, in invoke\n",
      "    return ctx.invoke(self.callback, **ctx.params)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\vanheesla\\AppData\\Local\\anaconda3\\envs\\OntoGPT\\Lib\\site-packages\\click\\core.py\", line 783, in invoke\n",
      "    return __callback(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\vanheesla\\AppData\\Local\\anaconda3\\envs\\OntoGPT\\Lib\\site-packages\\ontogpt\\cli.py\", line 894, in convert\n",
      "    template_details = get_template_details(template=template)\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\vanheesla\\AppData\\Local\\anaconda3\\envs\\OntoGPT\\Lib\\site-packages\\ontogpt\\io\\template_loader.py\", line 62, in get_template_details\n",
      "    sv = SchemaView(path_to_template)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\vanheesla\\AppData\\Local\\anaconda3\\envs\\OntoGPT\\Lib\\site-packages\\linkml_runtime\\utils\\schemaview.py\", line 152, in __init__\n",
      "    schema = load_schema_wrap(schema)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\vanheesla\\AppData\\Local\\anaconda3\\envs\\OntoGPT\\Lib\\site-packages\\linkml_runtime\\utils\\schemaview.py\", line 84, in load_schema_wrap\n",
      "    schema = yaml_loader.load(path, target_class=SchemaDefinition, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\vanheesla\\AppData\\Local\\anaconda3\\envs\\OntoGPT\\Lib\\site-packages\\linkml_runtime\\loaders\\loader_root.py\", line 76, in load\n",
      "    results = self.load_any(*args, **kwargs)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\vanheesla\\AppData\\Local\\anaconda3\\envs\\OntoGPT\\Lib\\site-packages\\linkml_runtime\\loaders\\yaml_loader.py\", line 41, in load_any\n",
      "    data_as_dict = self.load_as_dict(source, base_dir=base_dir, metadata=metadata)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\vanheesla\\AppData\\Local\\anaconda3\\envs\\OntoGPT\\Lib\\site-packages\\linkml_runtime\\loaders\\yaml_loader.py\", line 27, in load_as_dict\n",
      "    data = self._read_source(source, base_dir=base_dir, metadata=metadata, accept_header=\"text/yaml, application/yaml;q=0.9\")\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\vanheesla\\AppData\\Local\\anaconda3\\envs\\OntoGPT\\Lib\\site-packages\\linkml_runtime\\loaders\\loader_root.py\", line 167, in _read_source\n",
      "    data = hbread(source, metadata, base_dir, accept_header)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\vanheesla\\AppData\\Local\\anaconda3\\envs\\OntoGPT\\Lib\\site-packages\\hbreader\\__init__.py\", line 260, in hbread\n",
      "    with hbopen(source, open_info, base_path, accept_header, is_actual_data, read_codec) as f:\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\vanheesla\\AppData\\Local\\anaconda3\\envs\\OntoGPT\\Lib\\site-packages\\hbreader\\__init__.py\", line 206, in hbopen\n",
      "    f = open(fname, encoding=read_codec if read_codec else 'utf-8')\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "FileNotFoundError: [Errno 2] No such file or directory: 'C:\\\\Users\\\\vanheesla\\\\AppData\\\\Local\\\\anaconda3\\\\envs\\\\OntoGPT\\\\Lib\\\\site-packages\\\\ontogpt\\\\templates\\\\output.yaml'\n"
     ]
    }
   ],
   "source": [
    "!ontogpt convert -t output -o output.jsonl -O jsonl ./Results/outputGPT3.5_sv.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usage: ontogpt convert [OPTIONS] INPUT\n",
      "\n",
      "  Convert output format.\n",
      "\n",
      "Options:\n",
      "  -m, --model TEXT                Model name to use, e.g. orca-mini-7b or\n",
      "                                  gpt-4. See all model names with ontogpt\n",
      "                                  list-models.\n",
      "  -t, --template TEXT             Template to use.  [required]\n",
      "  -o, --output FILENAME           Output file.\n",
      "  -O, --output-format [json|yaml|pickle|md|html|owl|turtle|jsonl|kgx]\n",
      "                                  Output format.\n",
      "  --help                          Show this message and exit.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:ontogpt.clients:llm_gpt4all module not found. GPT4All support will be disabled.\n",
      "WARNING:ontogpt.engines.knowledge_engine:GPT4All client not available. GPT4All support will be disabled.\n"
     ]
    }
   ],
   "source": [
    "!ontogpt convert --help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "OntoGPT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
